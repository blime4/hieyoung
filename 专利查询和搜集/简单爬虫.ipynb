{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36\"\n",
    "}\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page_num = int(630/50)+2\n",
    "url = \"http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=0&f=S&l=50&d=PTXT&p=2&S1=%22Plant+stand%22&Page=Next&OS=%22Plant+stand%22&RS=%22Plant+stand%22\"\n",
    "url_head = url.split(\"&p=\")[0]\n",
    "url_tail = url.split(\"&S1\")[1]\n",
    "page_a = []\n",
    "page_img = []\n",
    "page_href = []\n",
    "for i in range(1,page_num):\n",
    "    i_url = url_head + \"&p=\"+str(i) + \"&S1\" + url_tail\n",
    "#     print(i_url)\n",
    "    response = requests.get( i_url , headers=headers )\n",
    "    if response.status_code == 200:\n",
    "        print(\"[一切正常]---爬取到第\"+str(i)+\"页\")\n",
    "    else:\n",
    "        print(\"[出错]---错误状态码：\"+response.status_code)\n",
    "    html_doc = response.text\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    soup_tb = soup.find_all(\"table\")\n",
    "    for tb in soup_tb:\n",
    "        for tr in tb.find_all(\"tr\"):\n",
    "            valign_top = list(tr.find_all(\"td\",attrs={\"valign\":\"top\"}))\n",
    "            if len(valign_top)>=2:\n",
    "                num = valign_top[1].get_text().replace(\",\",\"\")\n",
    "                img = \"https://pdfpiw.uspto.gov/.piw?Docid=\"+str(num)\n",
    "                top = valign_top[2]\n",
    "                href = \"http://patft.uspto.gov/\"+str(top.a.get(\"href\"))\n",
    "                a = top.get_text()\n",
    "                a = a.replace(\"\\n\",\" \")\n",
    "                page_a.append(a)\n",
    "                page_img.append(img)\n",
    "                page_href.append(href)\n",
    "    time.sleep(5)\n",
    "page_dict = {\"标题\":page_a,\"专利链接\":page_href,\"图片链接\":page_img}\n",
    "page_df = pd.DataFrame(page_dict)\n",
    "page_df.to_excel(\"标题-专利链接-图片链接.xlsx\",index=None)\n",
    "print(\"已导出到--标题-专利链接-图片链接.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=page_change&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=0&f=S&l=50&TERM1=search_change&FIELD1=&co1=AND&TERM2=&FIELD2=&d=PTXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=page_change&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=0&f=S&l=50&TERM1=search_change&FIELD1=&co1=AND&TERM2=&FIELD2=&d=PTXT\"\n",
    "key_words = [\"Plant stand\",\"Hanging Plant\",\"Plant hanger\",\"plant holder\",\"wall plant\",\"plant caddy\"]\n",
    "key_words = [i.replace(\" \",\"+\") for i in key_words]\n",
    "key_words_urls = []\n",
    "for i in key_words:\n",
    "    key_words_url = url\n",
    "    key_words_urls.append(key_words_url.replace(\"search_change\",i))\n",
    "key_words_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "}\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[一切正常]---开始爬取 wall+plant\n",
      "一共441个论文，共9页\n",
      "[一切正常]---爬取到第2页\n",
      "[一切正常]---爬取到第3页\n",
      "[一切正常]---爬取到第4页\n",
      "[一切正常]---爬取到第5页\n",
      "[一切正常]---爬取到第6页\n",
      "[一切正常]---爬取到第7页\n",
      "[一切正常]---爬取到第8页\n",
      "[一切正常]---爬取到第9页\n",
      "[ ok ]---wall+plant\n",
      "[一切正常]---开始爬取 Plant+hanger\n",
      "一共127个论文，共3页\n",
      "[一切正常]---爬取到第2页\n",
      "[一切正常]---爬取到第3页\n",
      "[ ok ]---Plant+hanger\n",
      "[一切正常]---开始爬取 plant+holder\n",
      "一共249个论文，共5页\n",
      "[一切正常]---爬取到第2页\n",
      "[一切正常]---爬取到第3页\n",
      "[一切正常]---爬取到第4页\n",
      "[一切正常]---爬取到第5页\n",
      "[ ok ]---plant+holder\n",
      "全部导出完毕\n"
     ]
    }
   ],
   "source": [
    "url = \"http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=page_change&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=0&f=S&l=50&TERM1=search_change&FIELD1=&co1=AND&TERM2=&FIELD2=&d=PTXT\"\n",
    "# key_words = [\"plant caddy\",\"Plant stand\",\"Hanging Plant\",\"Plant hanger\",\"plant holder\",\"wall plant\"]\n",
    "key_words = [\"wall plant\",\"Plant hanger\",\"plant holder\"]\n",
    "\n",
    "key_words = [i.replace(\" \",\"+\") for i in key_words]\n",
    "\n",
    "# writer = pd.ExcelWriter(\"标题-专利链接-图片链接.xlsx\")\n",
    "for key_word in key_words:\n",
    "    page_a = []\n",
    "    page_img = []\n",
    "    page_href = []\n",
    "    key_words_url = url\n",
    "    key_words_url = key_words_url.replace(\"search_change\",str(key_word))\n",
    "    url_1 = key_words_url\n",
    "    url_1 = url_1.replace(\"page_change\",\"1\")\n",
    "    response = requests.get( url_1 , headers=headers )\n",
    "    if response.status_code == 200:\n",
    "        print(\"[一切正常]---开始爬取 \"+key_word)\n",
    "    else:\n",
    "        print(\"[出错]---错误状态码：\"+response.status_code)\n",
    "    html_doc = response.text\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    total_len = soup.find(\"body\").find_all(\"i\")[1].find_all(\"strong\")[2].get_text()\n",
    "    page_num = int(float(total_len)/50.5)+2\n",
    "    print(\"一共\"+total_len+\"个论文，共\"+str(page_num-1)+\"页\")\n",
    "    soup_tb = soup.find_all(\"table\")\n",
    "    for tb in soup_tb:\n",
    "        for tr in tb.find_all(\"tr\"):\n",
    "            valign_top = list(tr.find_all(\"td\",attrs={\"valign\":\"top\"}))\n",
    "            if len(valign_top)>=2:\n",
    "                num = valign_top[1].get_text().replace(\",\",\"\")\n",
    "                img = \"https://pdfpiw.uspto.gov/.piw?Docid=\"+str(num)\n",
    "                top = valign_top[2]\n",
    "                href = \"http://patft.uspto.gov/\"+str(top.a.get(\"href\"))\n",
    "                a = top.get_text()\n",
    "                a = a.replace(\"\\n\",\" \")\n",
    "                page_a.append(a)\n",
    "                page_img.append(img)\n",
    "                page_href.append(href)\n",
    "    time.sleep(60)\n",
    "    if page_num >= 2:\n",
    "        for i in range(2,page_num):\n",
    "            url_i = key_words_url\n",
    "            url_i = url_i.replace(\"page_change\",str(i))\n",
    "            response = requests.get( url_i , headers=headers )\n",
    "            if response.status_code == 200:\n",
    "                print(\"[一切正常]---爬取到第\"+str(i)+\"页\")\n",
    "            else:\n",
    "                print(\"[出错]---错误状态码：\"+response.status_code)\n",
    "            html_doc = response.text\n",
    "            response.close()\n",
    "            soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "            soup_tb = soup.find_all(\"table\")\n",
    "            for tb in soup_tb:\n",
    "                for tr in tb.find_all(\"tr\"):\n",
    "                    valign_top = list(tr.find_all(\"td\",attrs={\"valign\":\"top\"}))\n",
    "                    if len(valign_top)>=2:\n",
    "                        num = valign_top[1].get_text().replace(\",\",\"\")\n",
    "                        img = \"https://pdfpiw.uspto.gov/.piw?Docid=\"+str(num)\n",
    "                        top = valign_top[2]\n",
    "                        href = \"http://patft.uspto.gov/\"+str(top.a.get(\"href\"))\n",
    "                        a = top.get_text()\n",
    "                        a = a.replace(\"\\n\",\" \")\n",
    "                        page_a.append(a)\n",
    "                        page_img.append(img)\n",
    "                        page_href.append(href)\n",
    "            time.sleep(60)\n",
    "    page_dict = {\"标题\":page_a,\"专利链接\":page_href,\"图片链接\":page_img}\n",
    "    page_df = pd.DataFrame(page_dict)\n",
    "    page_df.to_excel(key_word+\".xlsx\",index=None)\n",
    "#     page_df.to_excel(writer,sheet_name=key_word,index=None)\n",
    "    print(\"[ ok ]---\"+key_word)\n",
    "print(\"全部导出完毕\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异步爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "# import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "}\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'await' outside async function (<ipython-input-34-09e4caaa91a3>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-09e4caaa91a3>\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    page_a = []\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'await' outside async function\n"
     ]
    }
   ],
   "source": [
    "def data_process(task):\n",
    "    page_a = []\n",
    "    page_img = []\n",
    "    page_href = []\n",
    "    print('在回调函数中，实现数据解析')\n",
    "    html_doc = task.result()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    total_len = soup.find(\"body\").find_all(\"i\")[1].find_all(\"strong\")[2].get_text()\n",
    "    page_num = int(float(total_len)/50.5)+2\n",
    "    print(\"一共\"+total_len+\"个论文，共\"+str(page_num-1)+\"页\")\n",
    "    soup_tb = soup.find_all(\"table\")\n",
    "    for tb in soup_tb:\n",
    "        for tr in tb.find_all(\"tr\"):\n",
    "            valign_top = list(tr.find_all(\"td\",attrs={\"valign\":\"top\"}))\n",
    "            if len(valign_top)>=2:\n",
    "                num = valign_top[1].get_text().replace(\",\",\"\")\n",
    "                img = \"https://pdfpiw.uspto.gov/.piw?Docid=\"+str(num)\n",
    "                top = valign_top[2]\n",
    "                href = \"http://patft.uspto.gov/\"+str(top.a.get(\"href\"))\n",
    "                a = top.get_text()\n",
    "                a = a.replace(\"\\n\",\" \")\n",
    "                page_a.append(a)\n",
    "                page_img.append(img)\n",
    "                page_href.append(href)\n",
    "    page_dict = {\"标题\":page_a,\"专利链接\":page_href,\"图片链接\":page_img}\n",
    "    page_df = pd.DataFrame(page_dict)\n",
    "    page_df.to_csv(key_word+\".csv\",index=None)\n",
    "    await asyncio.sleep(60)\n",
    "    if page_num >= 2:\n",
    "        for i in range(2,page_num):\n",
    "            page_a = []\n",
    "            page_img = []\n",
    "            page_href = []\n",
    "            url_i = key_words_url\n",
    "            url_i = url_i.replace(\"page_change\",str(i))\n",
    "            c = get_page(url)\n",
    "            task = asyncio.ensure_future(c)\n",
    "            #给任务对象绑定回调函数用于解析响应数据\n",
    "            task.add_done_callback(callback)\n",
    "            tasks.append(task)\n",
    "            response = requests.get( url_i , headers=headers )\n",
    "            if response.status_code == 200:\n",
    "                print(\"[一切正常]---爬取到第\"+str(i)+\"页\")\n",
    "            else:\n",
    "                print(\"[出错]---错误状态码：\"+response.status_code)\n",
    "            html_doc = response.text\n",
    "            response.close()\n",
    "            soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "            soup_tb = soup.find_all(\"table\")\n",
    "            for tb in soup_tb:\n",
    "                for tr in tb.find_all(\"tr\"):\n",
    "                    valign_top = list(tr.find_all(\"td\",attrs={\"valign\":\"top\"}))\n",
    "                    if len(valign_top)>=2:\n",
    "                        num = valign_top[1].get_text().replace(\",\",\"\")\n",
    "                        img = \"https://pdfpiw.uspto.gov/.piw?Docid=\"+str(num)\n",
    "                        top = valign_top[2]\n",
    "                        href = \"http://patft.uspto.gov/\"+str(top.a.get(\"href\"))\n",
    "                        a = top.get_text()\n",
    "                        a = a.replace(\"\\n\",\" \")\n",
    "                        page_a.append(a)\n",
    "                        page_img.append(img)\n",
    "                        page_href.append(href)\n",
    "            page_dict = {\"标题\":page_a,\"专利链接\":page_href,\"图片链接\":page_img}\n",
    "            page_df = pd.DataFrame(page_dict)\n",
    "            page_df.to_csv(key_word+\".csv\",index=None,header=False,mode=\"a\")\n",
    "            await asyncio.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_page(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with await session.get(url=url) as response:\n",
    "            page_text = await response.text() #read()  json()\n",
    "            return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "loop = asyncio.get_event_loop()\n",
    "for url in urls:\n",
    "    c = get_page(url)\n",
    "    task = asyncio.ensure_future(c)\n",
    "    #给任务对象绑定回调函数用于解析响应数据\n",
    "    task.add_done_callback(callback)\n",
    "    tasks.append(task)\n",
    "loop.run_until_complete(asyncio.wait(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=page_change&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=0&f=S&l=50&TERM1=search_change&FIELD1=&co1=AND&TERM2=&FIELD2=&d=PTXT\"\n",
    "key_words = [\"wall plant\",\"Plant hanger\",\"plant holder\"]\n",
    "key_words = [i.replace(\" \",\"+\") for i in key_words]\n",
    "tasks = []\n",
    "loop = asyncio.get_event_loop()\n",
    "# writer = pd.ExcelWriter(\"标题-专利链接-图片链接.xlsx\")\n",
    "for key_word in key_words:\n",
    "    key_words_url = url\n",
    "    key_words_url = key_words_url.replace(\"search_change\",str(key_word))\n",
    "    url_1 = key_words_url\n",
    "    url_1 = url_1.replace(\"page_change\",\"1\")\n",
    "    response = get_page(url_1)\n",
    "    task = asyncio.ensure_future(c)\n",
    "    task.add_done_callback(data_process)\n",
    "    tasks.append(task)\n",
    "loop.run_until_complete(asyncio.wait(tasks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
